{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram Mixture model\n",
    "\n",
    "\n",
    "##### The model :\n",
    "First, we sample the topic of a document as a one-hot vector, from a multinomial of size $K$.\n",
    "- $z \\sim \\mathcal{M}(1, (\\pi_1, \\dots, \\pi_K)), z\\in\\{0, 1\\}^K$\n",
    "- $p(z)=\\prod_{k=1}^K\\pi_k^{z_k}$\n",
    "\n",
    "Once the topic $z^i$ is selected for document $i$, we sample the documents $N$ words from a multinomial of size $d$, the vocabulary size.\n",
    "- $w^i_n~|~\\{z^i_k = 1\\} \\sim \\mathcal{M}(1, (b_{1k},\\dots,b_{dk}))$\n",
    "\n",
    "In plate notations :\n",
    "<img src=\"img/unigram_mixture_new.png\" alt=\"unigram mixture\" width=\"200\"/>\n",
    "\n",
    "Finally :\n",
    "$\\displaystyle{p(w^i, z^i) = \\prod_{j=1}^d\\prod_{k=1}^K \\prod_{n=1}^{N}(b_{jk}\\pi_k)^{w^i_n(j)z_k}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0:\n",
    "Our first goal will be to sample documents according to this generating process. What is an alternative to sampling $N$ multinomials of size $1$ ? Write the new joint probability distribution and draw the new model in plate notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "Sample $M=100$ documents of $N=30$ words from the given $\\pi$ and $b$ in the next cell. We have $d=3$ words, but $K=6$ topics.\n",
    "Write a function _gen_\\__corpus_ that takes all the required parameters as input, and returns a (M,d) array containg the $x^{(i)}$, as well as a (M,) array containing the topics associated with these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_docs = 400\n",
    "doc_length = 30\n",
    "n_topics = 6\n",
    "n_tokens = 3\n",
    "\n",
    "topic_proba = np.array([1./n_topics] * n_topics)\n",
    "word_proba = np.array([\n",
    "    [0.8, 0.1, 0.1],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.1, 0.1, 0.8],\n",
    "    [0.45, 0.45, 0.1],\n",
    "    [0.1, 0.45, 0.45],\n",
    "    [0.45, 0.1, 0.45]\n",
    "])\n",
    "\n",
    "\n",
    "def gen_corpus(n_docs, n_topics, n_tokens, doc_length, topic_proba, word_proba):\n",
    "    \"\"\"\n",
    "    n_docs: number of documents, S\n",
    "    n_topics: number of topics, K\n",
    "    doc_length: number of words per documents, N\n",
    "    topic_proba: (K,) array containing the pi_k\n",
    "    word_proba: (K, d) array containing the b_{k,j}.\n",
    "    \"\"\"\n",
    "    corpus = np.empty((n_docs, n_tokens), dtype='int32')\n",
    "    topics = # sample the topic of each documents\n",
    "\n",
    "    # Select corresponding multinomials\n",
    "    for i_doc, topic in enumerate(topics):\n",
    "        pass  # Sample the count of words in each documents\n",
    "    return corpus, topics\n",
    "\n",
    "corpus, topics = gen_corpus(n_docs, n_topics, n_tokens, doc_length, topic_proba, word_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data :\n",
    "In the next cell, we represent this corpus. We will use the three vertices of a triangle to represent our three tokens. A document will be represented as a convex combination of the three vertices. We will color each document according to their topic (red, green, blue and black).\n",
    "Finally, each row in $b$ giving the token probability given the topic will also be visualized.\n",
    "\n",
    "### Question 2:\n",
    "Run the next cell to visualize the dataset. Explain what is shown in the figure produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "# Compute the triangle vertices using complex roots of 1\n",
    "vertex_ids = np.array(range(n_tokens))\n",
    "vertices = np.vstack([\n",
    "    np.cos(2 * math.pi * vertex_ids / n_tokens),\n",
    "    np.sin(2 * math.pi * vertex_ids / n_tokens)]\n",
    ").T\n",
    "\n",
    "# Plot the triangle\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "polygon = Polygon(vertices, fill=False, linewidth=2, linestyle='--')\n",
    "ax.add_patch(polygon)\n",
    "ax.set_xlim([-1.1, 1.1])\n",
    "ax.set_ylim([-1.1, 1.1])\n",
    "\n",
    "# Compute a S x 2 array containing documents as linear combination of our vertices\n",
    "\n",
    "linear_combinations = (corpus / np.sum(corpus, axis=1)[:, None]) @ vertices  \n",
    "colors = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 0], [1, 1, 0], [0, 1, 1]])\n",
    "\n",
    "plt.scatter(\n",
    "    linear_combinations[:, 0], linear_combinations[:, 1], c=np.array(topics) @ colors,\n",
    "    marker='o', s=500, alpha=0.6\n",
    ")\n",
    "\n",
    "centers = word_proba @ vertices\n",
    "\n",
    "plt.scatter(\n",
    "    centers[:, 0], centers[:, 1], c=colors,\n",
    "    marker='X', s=500, alpha=1, edgecolor=(0, 0, 0, 1), linewidths=2\n",
    ")\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe documents $(x^{(i)})_{i=1..M}$, and apply EM for $t=1\\dots T$.\n",
    "\n",
    "\n",
    "##### E-Step:\n",
    "- $\\displaystyle{p(z_k=1~|~x^{(i)};b^{(t-1)};\\pi^{(t-1)}) = \\frac{\\pi^{(t-1)}_k \\prod_{j=1}^d(b_{jk}^{(t-1)})^{x_j}}{\\sum_{k'=1}^K\\pi^{(t-1)}_{k'}\\prod_{j=1}^d(b_{jk'}^{(t-1)})^{x_j}}}$\n",
    "- $q^{(t)}_{ik} = \\mathbb{E}[z_k~|~x^{(i)};b^{(t)};\\pi^{(t)}]$\n",
    "\n",
    "\n",
    "##### M-Step:\n",
    "- $\\displaystyle{b_{jk}^{(t)} = \\frac{\\sum_{i}x_j^{(i)}q_{ik}^{(t)}}{\\sum_{i,j'}x_{j'}^{(i)}q_{ik}^{(t)}}}$ and $\\displaystyle{\\pi_{k}^{(t)} = \\frac{\\sum_{i}q_{ik}^{(t)}}{\\sum_{i,k'}q_{ik'}^{(t)}}}$\n",
    "\n",
    "\n",
    "### Question 3:\n",
    "1. Show the formula for $b^{(t)}_{jk}$ starting from the expected complete log-likelihood :\n",
    "$$\\mathbb{E}_{q_i^{(t)}}[\\log(p(X, Z;b,\\pi)]=\\sum_{i,j,k}x_j^{(i)}q_{ik}^{(t)}\\log(b_{jk}) + \\sum_{i,k}q_{ik}^{(t)}\\log(\\pi_k)$$\n",
    "2. What problem can happen when computing the $q_{i,k}$ in the E step ?\n",
    "3. Write code for logsumexp which computes $\\log(\\sum_i\\exp(x_i))$, beware of (2.). Write a test that shows the implementation works as expected. How can we use this function in the E step ?\n",
    "4. Fill in the code for the E and M steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    \"\"\"\n",
    "    x: a (n, m) array.\n",
    "    return: log(sum(exp(x_i,:))) for all i, avoiding numerical underflow\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# test that our function does what it should on simple inputs\n",
    "x = np.array([[1, 2, 3], [2, 2, 2]]) \n",
    "print(logsumexp(x), np.log(np.sum(np.exp(x), axis=1)))\n",
    "\n",
    "# write a test case in which the basic method overflows and yours doesn't\n",
    "x = np.array([[...], [...]]) \n",
    "print(logsumexp(x), np.log(np.sum(np.exp(x), axis=1))) # \n",
    "\n",
    "\n",
    "def E_step(corpus, learned_topic_proba, learned_word_proba):\n",
    "    \"\"\"\n",
    "    corpus : (S, n_tokens) array containing the counts of words in each document\n",
    "    learned_topic_proba : (n_topics, ) array containing the probability of each topic\n",
    "    learned_topic_proba : (n_topics, n_token) array containing the probability of each word for each topic\n",
    "    returns : doc_topic_proba : (S, n_topicdlhkfiidgninkhvulrjvurlvhrdekcur) the q_ik\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def M_step(corpus, doc_topic_probas):\n",
    "    \"\"\"\n",
    "    corpus : (S, n_tokens) array containing the counts of words in each document\n",
    "    doc_topic_probas : (S, n_topics) array containing q_ik computed in the E-step\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Likelihood\n",
    "It is convenient to visualize the log-likelihood of our data during training to make sure it is being maximized.\n",
    "\n",
    "### Question 4:\n",
    "1. Implement computation of the log-likelihood in the next cell. <em>log_likelihood_qik</em> will be used later in the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_E_step(corpus, doc_topic_probas, learned_word_proba, learned_topic_proba):\n",
    "    \"\"\"\n",
    "    corpus : (S, n_tokens) array containing the counts of words in each document\n",
    "    doc_topic_probas : (S, n_topics) array containing E_q[z] computed in the E-step\n",
    "    learned_topic_proba : (n_topics, ) array containing the probability of each topic\n",
    "    learned_word_proba : (n_topics, n_token) array containing the probability of each word for each topic\n",
    "    \"\"\"\n",
    "    first_term = corpus @ np.log(learned_word_proba.T)\n",
    "    first_term[np.isnan(first_term)] = 0\n",
    "    first_term *= doc_topic_probas\n",
    "    \n",
    "    second_term = doc_topic_probas @ np.log(learned_topic_proba)[:, None]\n",
    "    entropy = np.log(doc_topic_probas) * doc_topic_probas\n",
    "    entropy[np.isnan(entropy)] = 0\n",
    "    return (np.sum(first_term) + np.sum(second_term) - np.sum(entropy)) / corpus.shape[0]\n",
    "\n",
    "def log_likelihood(corpus, learned_word_proba, learned_topic_proba):\n",
    "    \"\"\"\n",
    "    corpus : (S, n_tokens) array containing the counts of words in each document\n",
    "    learned_topic_proba : (n_topics, ) array containing the probability of each topic\n",
    "    learned_word_proba : (n_topics, n_token) array containing the probability of each word for each topic\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running EM :\n",
    "We can now run the Expectation-Maximization algorithm on our corpus.\n",
    "We will initialize the algorithm by setting $\\pi_k = 1/K$.\n",
    "For $b_{k:}$, we use the proportions of random documents.\n",
    "Then we run the algorithm for a few iterations.\n",
    "\n",
    "At completion, we plot the log likelihood curve for train and valid.\n",
    "\n",
    "### Question 5:\n",
    "- Complete the following code to run EM, plot the train and valid log-likelihood. Also plot the log-likelihood obtain from the true parameters <em>topic_proba</em> and <em>word_proba</em>. Comment.\n",
    "- Compute the train log-likelihood after the E-step. Compare the curve of this log-likelihood and train_ll_E_step. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_corpus, valid_topics = gen_corpus(1000, n_topics, n_tokens, doc_length, topic_proba, word_proba)\n",
    "\n",
    "n_iter = 20\n",
    "n_learned_topics = 6\n",
    "\n",
    "# Initialize the learned parameter\n",
    "# uniform topic proba\n",
    "learned_topic_proba = np.ones((n_learned_topics,)) / n_learned_topics\n",
    "\n",
    "# word proba taken from random documents + some constant to avoid zeroes\n",
    "learned_word_proba = corpus[np.random.permutation(n_docs)[:n_learned_topics]] + 1e-3\n",
    "learned_word_proba = learned_word_proba / np.sum(learned_word_proba, axis=1)[:, None]\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "train_ll = []\n",
    "valid_ll = []\n",
    "for i in range(n_iter):\n",
    "    # step of the EM algorithm\n",
    "    # compute the log-likelihoods and append to train_ll, valid_ll\n",
    "    \n",
    "# plot train_ll, valid_ll\n",
    "\n",
    "# compute valid log-likelihood for real parameters of model and plot\n",
    "    \n",
    "plt.xlabel(\"EM-iteration\")\n",
    "plt.ylabel(\"Data log-likelihood\")\n",
    "plt.legend(bbox_to_anchor=(1,1), loc=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6:\n",
    "Plot the learned word_proba along the dataset and original word_probas. Repeat the process of learning / visualizing, comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the triangle\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "polygon = Polygon(vertices, fill=False, linewidth=2, linestyle='--')\n",
    "ax.add_patch(polygon)\n",
    "ax.set_xlim([-1.1, 1.1])\n",
    "ax.set_ylim([-1.1, 1.1])\n",
    "\n",
    "# Compute a S x 2 array containing documents as linear combination of our vertices\n",
    "linear_combinations = (corpus / np.sum(corpus, axis=1)[:, None]) @ vertices  \n",
    "\n",
    "plt.scatter(\n",
    "    linear_combinations[:, 0], linear_combinations[:, 1], c=np.array(topics) @ colors,\n",
    "    marker='o', s=500, alpha=0.2\n",
    ")\n",
    "\n",
    "# plot real word_proba\n",
    "\n",
    "centers = word_proba @ vertices\n",
    "\n",
    "plt.scatter(\n",
    "    centers[:, 0], centers[:, 1], c=colors,\n",
    "    marker='X', s=500, alpha=1, edgecolor=(0, 0, 0, 1), linewidths=2\n",
    ")\n",
    "\n",
    "# plot the learned word_proba.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger synthetic dataset\n",
    "\n",
    "Now, we will generate a more realistic synthetic dataset. We will have $1000$ documents of $200$ words each, $10$ topics and $50$ different tokens.\n",
    "We will randomly generate our generative model's parameters with uniform <em>topic_proba</em> and <em>word_proba</em> coming from a [Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution) distribution with parameter $\\alpha=0.5$.\n",
    "\n",
    "### Question 6:\n",
    "- How can we select the correct number of topics for a dataset ?\n",
    "- Implement this idea in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_docs = 100\n",
    "n_valid_docs = 1000\n",
    "doc_length = 200\n",
    "n_topics = 10\n",
    "n_tokens = 50\n",
    "alpha = 0.5\n",
    "\n",
    "\n",
    "def gen_bigger_corpus(\n",
    "    n_train_docs, n_valid_docs, doc_length,\n",
    "    n_topics, n_tokens, alpha\n",
    "):\n",
    "    topic_proba = np.array([1./n_topics] * n_topics)\n",
    "    word_proba = np.random.dirichlet([alpha] * n_tokens, size=n_topics)\n",
    "\n",
    "    corpus, topics = gen_corpus(n_train_docs, n_topics, n_tokens, doc_length, topic_proba, word_proba)\n",
    "    valid_corpus, valid_topics = gen_corpus(n_valid_docs, n_topics, n_tokens, doc_length, topic_proba, word_proba)\n",
    "    return topic_proba, word_proba, corpus, valid_corpus\n",
    "\n",
    "# Implement your method here.\n",
    "# Use the previous function to generate your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application to Text\n",
    "This model is well suited to represent text. In the following, we will apply our EM algorithm to discover topics in the _newsgroup 20_ corpus.\n",
    "\n",
    "As is standard in Natural Language Processing, we will first apply a few pre-processing steps to the corpus as described [here](https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925).\n",
    "\n",
    "Make sure you have installed the following libraries into your conda environment : \n",
    "- gensim\n",
    "- nltk\n",
    "- scikit-learn\n",
    "\n",
    "First, we download the dataset, and wordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = {\n",
    "    'train': fetch_20newsgroups(subset='train', shuffle = True),\n",
    "    'test': fetch_20newsgroups(subset='test', shuffle = True)\n",
    "}\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Preprocessing :\n",
    "We use the same pre-processing as in the blog-post :\n",
    "- **Tokenization** : Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "- Words that have fewer than 3 characters are removed.\n",
    "- All **stopwords** are removed.\n",
    "- Words are **lemmatized** - third person is changed to first person, all verbs are changed into present tense\n",
    "- Words are **stemmed** - words are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result\n",
    "\n",
    "preprocess(\"The quick brown fox jumped over the lazy dogs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary making:\n",
    "Each unique element in the output of _preprocess_ is a token. We need to convert these into unique ids to run our algorithm. This is called building a **dictionary**. The library _gensim_ has a helper function to help us do this.\n",
    "\n",
    "### Question 7:\n",
    "- Why is all this preprocessing useful ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = {s: [] for s in ['train', 'test']}\n",
    "# This may take a while. We are processing the entire dataset.\n",
    "for s in ['train', 'test']:\n",
    "    for doc in newsgroups[s].data:\n",
    "        processed_docs[s].append(preprocess(doc))\n",
    "    \n",
    "dictionary = gensim.corpora.Dictionary(processed_docs['train'])\n",
    "print(f\"{len(dictionary)} unique tokens before filtering\")\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1)\n",
    "print(f\"{len(dictionary)} unique tokens after filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform our corpus of text to lists of counts of tokens\n",
    "bow_corpus = {\n",
    "    s: [dictionary.doc2bow(doc) for doc in processed_docs[s]]\n",
    "    for s in ['train', 'test']\n",
    "}\n",
    "\n",
    "n_tokens = len(dictionary)\n",
    "text_corpus = {'train': None, 'test': None}\n",
    "for s in text_corpus.keys():\n",
    "    n_docs = len(processed_docs[s])\n",
    "    text_corpus[s] = np.zeros((n_docs, n_tokens), dtype='int32')\n",
    "    for d, bow in enumerate(bow_corpus[s]):\n",
    "        for (id, value) in bow:\n",
    "            text_corpus[s][d, id] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model learning\n",
    "Now that the dataset is ready run EM on it, using $5$ topics. Since the dataset is bigger, this could take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New E_step using bag of words rather than dense count arrays\n",
    "def E_step_sparse(corpus, learned_topic_proba, learned_word_proba):\n",
    "    n_topics = learned_topic_proba.shape[0]\n",
    "    res = np.zeros((len(corpus), n_topics))\n",
    "    for d, tuples in enumerate(corpus):\n",
    "        maxi = np.finfo('float64').min\n",
    "        for topic in range(n_topics):\n",
    "            cur = np.log(learned_topic_proba[topic])\n",
    "            for tok_id, v in tuples:\n",
    "                tok_prob = learned_word_proba[topic, tok_id]\n",
    "                cur += np.log(tok_prob) * v\n",
    "            res[d, topic] = cur\n",
    "            maxi = max(maxi, cur)\n",
    "        total = 0\n",
    "        for topic in range(n_topics):\n",
    "            total += np.exp(res[d, topic] - maxi)\n",
    "            res[d, topic] = np.exp(res[d, topic] - maxi)\n",
    "        res[d, :] /= total\n",
    "\n",
    "    return res\n",
    "\n",
    "n_iter = 10\n",
    "n_topics = 5\n",
    "n_docs = len(bow_corpus['train'])\n",
    "\n",
    "\n",
    "\n",
    "learned_topic_proba = np.ones((n_topics,)) / n_topics\n",
    "learned_word_proba = np.empty((n_topics, n_tokens), dtype='float32')\n",
    "# initialize topic probabilities with random documents\n",
    "permutation = np.random.permutation(n_docs)\n",
    "batch_size = n_docs // n_topics\n",
    "for t in range(n_topics):\n",
    "    stats = np.sum(text_corpus['train'][permutation[t * batch_size: (t+1) * batch_size]], axis=0)\n",
    "    learned_word_proba[t] = stats / np.sum(stats)\n",
    "\n",
    "train_ll = []\n",
    "valid_ll = []\n",
    "for i in range(n_iter):\n",
    "    # E-step\n",
    "    doc_topic_probas = E_step_sparse(bow_corpus['train'], learned_topic_proba, learned_word_proba)\n",
    "\n",
    "    # E-step for valid\n",
    "    valid_topic_probas = E_step_sparse(bow_corpus['test'], learned_topic_proba, learned_word_proba)\n",
    "\n",
    "    # M-step\n",
    "    learned_topic_proba, learned_word_proba = M_step(text_corpus['train'], doc_topic_probas)\n",
    "\n",
    "    train_ll.append(log_likelihood(text_corpus['train'], learned_word_proba, learned_topic_proba))    \n",
    "    valid_ll.append(log_likelihood(text_corpus['test'], learned_word_proba, learned_topic_proba))\n",
    "    \n",
    "plt.figure(dpi=150)\n",
    "p, = plt.plot(range(len(train_ll)), train_ll, '-', label=f\"{n_topics} topics\")\n",
    "plt.plot(range(len(valid_ll)), valid_ll, '--', c=p.get_color())\n",
    "    \n",
    "plt.xlabel(\"EM-iteration\")\n",
    "plt.ylabel(\"Data log-likelihood\")\n",
    "plt.legend(bbox_to_anchor=(1,1), loc=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the topics\n",
    "\n",
    "Now that we've run EM, we can print the most frequent terms associated to each topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(learned_topic_proba)\n",
    "for t in range(n_topics):\n",
    "    most_frequent = np.argsort(learned_word_proba[t])[-10:]\n",
    "    print(f\"------- Topic {t} ------\")\n",
    "    for w in most_frequent:\n",
    "        print(f\"\\t -- {dictionary[int(w)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "The [perplexity](https://en.wikipedia.org/wiki/Perplexity) of a model $q$ on a test sample $(x_s)$ is given by :\n",
    "$$2^{-\\frac{1}{S}\\sum_{s=1}^S\\log_2(q(x_s))}$$\n",
    "\n",
    "### Question 8 :\n",
    "Relate this quantity to one of the quantities we've used in this practical session.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation\n",
    "A [more complicated graphical model](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) allows documents to come from mixtures of topics. The library _gensim_ can learn this model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(\n",
    "    bow_corpus['train'], num_topics = 5, id2word = dictionary, passes = 10, workers = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Latent Dirichlet Allocation model is linked to the following generative process (multinomial PCA):\n",
    "- Each document now has a mixture of topic $\\theta_i$\n",
    "- For each word, first draw a topic from $\\theta_i$, then draw a word from this topic.\n",
    "\n",
    "### Bonus Questions :\n",
    "- Write the conditional probability distributions of the topic and the words.\n",
    "- Derive the E and M step for this model.\n",
    "- In the code implemented for question 6, what happens when we increase the number of train documents ? Comment.\n",
    "- What can you notice regarding the computations done in the E-M algorithm for this practical session ? How would you implement this algorithm in a distributed fashion ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
